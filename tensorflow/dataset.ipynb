{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Dataset\n",
    "\n",
    "[TensorFlow Datasets](https://www.tensorflow.org/datasets/overview)\n",
    "\n",
    "### tf.data.Dataset\n",
    "\n",
    "tf.data.Dataset API is the first and foremost API you should understand when using TensorFlow. When I started using TensorFlow, it was quite hard to understand what it is and I was stuck to modeling and testing. However, if you don't understand the tf.data.Dataset you cannot create your own dataset for modeling or testing. \n",
    "\n",
    "According the the official documentation, tf.data.Dataset API provides below three things: \n",
    "\n",
    "1. Create a source dataset from your input data.\n",
    "2. Apply dataset transformations to preprocess the data.\n",
    "3. Iterate over the dataset and process the elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "def import_mnist_dataset(log=False):\n",
    "\n",
    "    fashion_mnist = keras.datasets.fashion_mnist\n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "    if log:\n",
    "        print(\"type of images  : \", type(train_images))\n",
    "        print(\"shape of images : \", train_images.shape)\n",
    "        print(\"type of label   : \", type(train_labels))\n",
    "        print(\"shape of label  : \", train_labels.shape)\n",
    "\n",
    "        print(\"type of images  : \", type(test_images))\n",
    "        print(\"shape of images : \", test_images.shape)\n",
    "        print(\"type of label   : \", type(test_labels))\n",
    "        print(\"shape of label  : \", test_labels.shape)\n",
    "\n",
    "        print(\"sampel Image\")\n",
    "        plt.imshow(train_images[0])\n",
    "        \n",
    "    return (train_images, train_labels), (test_images, test_labels)\n",
    "\n",
    "\n",
    "def download_iris_dataset(log=False):\n",
    "    train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "    train_dataset_file_path = tf.keras.utils.get_file(\n",
    "        fname=os.path.basename(train_dataset_url),\n",
    "        origin=train_dataset_url\n",
    "    )\n",
    "\n",
    "    if log:\n",
    "        print(\"Local copy of the dataset file: {}\".format(train_dataset_file_path))\n",
    "\n",
    "        data = pd.read_csv(train_dataset_file_path)\n",
    "        display(data.head())\n",
    "        \n",
    "    return train_dataset_file_path\n",
    "\n",
    "\n",
    "def parse_iris_dataset(train_dataset_file_path, log=False, image_display=True):\n",
    "    \"\"\"\n",
    "    tf.data.experimental.make_csv_dataset()\n",
    "    https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # column order in CSV file\n",
    "    column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "    feature_names = column_names[:-1]\n",
    "    label_name = column_names[-1]\n",
    "    \n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataset = tf.data.experimental.make_csv_dataset(\n",
    "        train_dataset_file_path,\n",
    "        batch_size,\n",
    "        column_names=column_names,\n",
    "        label_name=label_name,\n",
    "        num_epochs=1)\n",
    "    \n",
    "    # Extract first batch.\n",
    "    # As batch_size = 32, train_dataset is iterative.\n",
    "    features, labels = next(iter(train_dataset))\n",
    "\n",
    "    # Display scatter plot of the data.\n",
    "    if image_display:\n",
    "        plt.scatter(features['petal_length'],\n",
    "                    features['sepal_length'],\n",
    "                    c=labels,\n",
    "                    cmap='viridis')\n",
    "\n",
    "        plt.xlabel(\"Petal length\")\n",
    "        plt.ylabel(\"Sepal length\")\n",
    "        plt.show()\n",
    "    \n",
    "    if log:\n",
    "        print(\"Features: {}\".format(feature_names))\n",
    "        print(\"Label: {}\".format(label_name))\n",
    "\n",
    "        print(\"Type  : \", type(train_dataset))\n",
    "        \n",
    "        print(\"features : \", features[\"petal_length\"])\n",
    "        print(\"labels   : \", labels)\n",
    "    \n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def execute():\n",
    "\n",
    "    ds = download_iris_dataset()\n",
    "    parse_iris_dataset(ds, log=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_mnist_dataset_info(log=False):\n",
    "    ds, info = tfds.load('mnist', split='train', shuffle_files=True, with_info=True)\n",
    "    return ds, info\n",
    "\n",
    "\n",
    "def display_dataset(ds):\n",
    "    \"\"\"Check inside of dataset.\"\"\"\n",
    "    for d in ds.take(1):\n",
    "        print(list(d.keys()))\n",
    "        image = d[\"image\"]\n",
    "        label = d[\"label\"]\n",
    "        print(image.shape, label)\n",
    "\n",
    "        \n",
    "def import_dataset_as_numpy():\n",
    "    # as_supervised=True: Output 2-tuple structure (input, label)\n",
    "    ds = tfds.load(\"mnist\", split=\"train\", as_supervised=True)\n",
    "    for image, label in tfds.as_numpy(ds.take(1)):\n",
    "        print(type(image), type(label), label)\n",
    "\n",
    "\n",
    "def visualize_dataset():\n",
    "    \"\"\"Visualize mnist dataset.\n",
    "\n",
    "    tfds.visualization.show_examples()\n",
    "    https://www.tensorflow.org/datasets/api_docs/python/tfds/visualization/show_examples\n",
    "    \"\"\"\n",
    "    ds, info = tfds.load(\"mnist\", split=\"train\", with_info=True)\n",
    "    fig = tfds.show_examples(ds, info)\n",
    "    \n",
    "def cats_and_dog_dataset_load(log=False):\n",
    "    \"\"\"Load dataset\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    (raw_train, raw_validation, raw_test), metadata = load_training_dataset(log=True)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Each data is tf.data.Dataset.\n",
    "    # Metadata is info data.\n",
    "    (raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
    "        'cats_vs_dogs',\n",
    "        split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "        with_info=True,\n",
    "        as_supervised=True,\n",
    "    )\n",
    "    \n",
    "    if log:\n",
    "        print(\"raw_train : \", raw_train)\n",
    "        print(\"Type      : \", type(raw_train))\n",
    "    \n",
    "    return (raw_train, raw_validation, raw_test), metadata\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
