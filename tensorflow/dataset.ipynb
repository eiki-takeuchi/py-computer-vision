{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Dataset\n",
    "\n",
    "[TensorFlow Datasets](https://www.tensorflow.org/datasets/overview)\n",
    "\n",
    "### tf.data.Dataset\n",
    "\n",
    "tf.data.Dataset API is the first and foremost API you should understand when using TensorFlow. When I started using TensorFlow, it was quite hard to understand what it is and I was stuck to modeling and testing. However, if you don't understand the tf.data.Dataset you cannot create your own dataset for modeling or testing. \n",
    "\n",
    "According the the official documentation, tf.data.Dataset API provides below three things: \n",
    "\n",
    "1. Create a source dataset from your input data.\n",
    "2. Apply dataset transformations to preprocess the data.\n",
    "3. Iterate over the dataset and process the elements.\n",
    "\n",
    "### keras.preprocessing\n",
    "\n",
    "keras.preprocessing API is to load and preprocess data. \n",
    "\n",
    "keras.preprocessing.image is a set of tools for real-time data augmentation on image data. \n",
    "\n",
    "[Load using keras.preprocessing](https://www.tensorflow.org/tutorials/load_data/images#load_using_keraspreprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.compat.v2 as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pathlib\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def import_mnist_dataset(log=False):\n",
    "\n",
    "    fashion_mnist = keras.datasets.fashion_mnist\n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "    if log:\n",
    "        print(\"type of images  : \", type(train_images))\n",
    "        print(\"shape of images : \", train_images.shape)\n",
    "        print(\"type of label   : \", type(train_labels))\n",
    "        print(\"shape of label  : \", train_labels.shape)\n",
    "\n",
    "        print(\"type of images  : \", type(test_images))\n",
    "        print(\"shape of images : \", test_images.shape)\n",
    "        print(\"type of label   : \", type(test_labels))\n",
    "        print(\"shape of label  : \", test_labels.shape)\n",
    "\n",
    "        print(\"sampel Image\")\n",
    "        plt.imshow(train_images[0])\n",
    "        \n",
    "    return (train_images, train_labels), (test_images, test_labels)\n",
    "\n",
    "\n",
    "def download_iris_dataset(log=False):\n",
    "    train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "    train_dataset_file_path = tf.keras.utils.get_file(\n",
    "        fname=os.path.basename(train_dataset_url),\n",
    "        origin=train_dataset_url\n",
    "    )\n",
    "\n",
    "    if log:\n",
    "        print(\"Local copy of the dataset file: {}\".format(train_dataset_file_path))\n",
    "\n",
    "        data = pd.read_csv(train_dataset_file_path)\n",
    "        display(data.head())\n",
    "        \n",
    "    return train_dataset_file_path\n",
    "\n",
    "\n",
    "def parse_iris_dataset(train_dataset_file_path, log=False, image_display=True):\n",
    "    \"\"\"\n",
    "    tf.data.experimental.make_csv_dataset()\n",
    "    https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # column order in CSV file\n",
    "    column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "    feature_names = column_names[:-1]\n",
    "    label_name = column_names[-1]\n",
    "    \n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataset = tf.data.experimental.make_csv_dataset(\n",
    "        train_dataset_file_path,\n",
    "        batch_size,\n",
    "        column_names=column_names,\n",
    "        label_name=label_name,\n",
    "        num_epochs=1)\n",
    "    \n",
    "    # Extract first batch.\n",
    "    # As batch_size = 32, train_dataset is iterative.\n",
    "    features, labels = next(iter(train_dataset))\n",
    "\n",
    "    # Display scatter plot of the data.\n",
    "    if image_display:\n",
    "        plt.scatter(features['petal_length'],\n",
    "                    features['sepal_length'],\n",
    "                    c=labels,\n",
    "                    cmap='viridis')\n",
    "\n",
    "        plt.xlabel(\"Petal length\")\n",
    "        plt.ylabel(\"Sepal length\")\n",
    "        plt.show()\n",
    "    \n",
    "    if log:\n",
    "        print(\"Features: {}\".format(feature_names))\n",
    "        print(\"Label: {}\".format(label_name))\n",
    "\n",
    "        print(\"Type  : \", type(train_dataset))\n",
    "        \n",
    "        print(\"features : \", features[\"petal_length\"])\n",
    "        print(\"labels   : \", labels)\n",
    "    \n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def execute():\n",
    "\n",
    "    ds = download_iris_dataset()\n",
    "    parse_iris_dataset(ds, log=True)\n",
    "\n",
    "\n",
    "def import_mnist_dataset_info(log=False):\n",
    "    ds, info = tfds.load('mnist', split='train', shuffle_files=True, with_info=True)\n",
    "    return ds, info\n",
    "\n",
    "\n",
    "def display_dataset(ds):\n",
    "    \"\"\"Check inside of dataset.\"\"\"\n",
    "    for d in ds.take(1):\n",
    "        print(list(d.keys()))\n",
    "        image = d[\"image\"]\n",
    "        label = d[\"label\"]\n",
    "        print(image.shape, label)\n",
    "\n",
    "        \n",
    "def import_dataset_as_numpy():\n",
    "    # as_supervised=True: Output 2-tuple structure (input, label)\n",
    "    ds = tfds.load(\"mnist\", split=\"train\", as_supervised=True)\n",
    "    for image, label in tfds.as_numpy(ds.take(1)):\n",
    "        print(type(image), type(label), label)\n",
    "\n",
    "\n",
    "def visualize_dataset():\n",
    "    \"\"\"Visualize mnist dataset.\n",
    "\n",
    "    tfds.visualization.show_examples()\n",
    "    https://www.tensorflow.org/datasets/api_docs/python/tfds/visualization/show_examples\n",
    "    \"\"\"\n",
    "    ds, info = tfds.load(\"mnist\", split=\"train\", with_info=True)\n",
    "    fig = tfds.show_examples(ds, info)\n",
    "    \n",
    "\n",
    "def cats_and_dog_dataset_load(log=False):\n",
    "    \"\"\"Load dataset\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    (raw_train, raw_validation, raw_test), metadata = load_training_dataset(log=True)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Each data is tf.data.Dataset.\n",
    "    # Metadata is info data.\n",
    "    (raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
    "        'cats_vs_dogs',\n",
    "        split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "        with_info=True,\n",
    "        as_supervised=True,\n",
    "    )\n",
    "    \n",
    "    if log:\n",
    "        print(\"raw_train : \", raw_train)\n",
    "        print(\"Type      : \", type(raw_train))\n",
    "    \n",
    "    return (raw_train, raw_validation, raw_test), metadata\n",
    "    \n",
    "\n",
    "def download_flower_dataset():\n",
    "    data_dir = tf.keras.utils.get_file(origin='https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "                                             fname='flower_photos', untar=True)\n",
    "    return pathlib.Path(data_dir)\n",
    "\n",
    "\n",
    "def load_images_with_image_generator(\n",
    "    data_dir=\"../images\", \n",
    "    class_names=[\"dogs\", \"cats\"], \n",
    "    batch_size=32,\n",
    "    img_size=224,\n",
    "    log=True):\n",
    "    \n",
    "    \"\"\"Image dataset loader\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data_dir    : str\n",
    "    class_names : list\n",
    "        If you set [\"dogs\", \"cats\"], data_dir should have those directories.\n",
    "        class_names is mapping to directory structure. \n",
    "        data_dir/\n",
    "            /cats \n",
    "            /dogs \n",
    "    batch_size  : int\n",
    "    img_size    : int\n",
    "    log         : bool\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    train_data_gen : keras_preprocessing.image.directory_iterator.DirectoryIterator\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    image_data_gen = load_images()\n",
    "    image_batch, label_batch = next(train_data_gen)\n",
    "    show_batch(image_batch, label_batch)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get total number of image files.\n",
    "    total_image_files = 0\n",
    "    for name in class_names:\n",
    "        class_path = os.path.join(data_dir, name)\n",
    "        print(class_path)\n",
    "\n",
    "        # Get number of files in directory.\n",
    "        path, dirs, files = next(os.walk(class_path))\n",
    "        total_image_files += len(files)\n",
    "        \n",
    "    if log:\n",
    "        print(\"image_count : \", total_image_files)\n",
    "\n",
    "    # Set up parameters.\n",
    "    STEPS_PER_EPOCH = np.ceil(total_image_files/batch_size)\n",
    "\n",
    "    # Generate ImageDataGenerator.\n",
    "    # The 1./255 is to convert from uint8 to float32 in range [0,1].\n",
    "    image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Load Images.\n",
    "    train_data_gen = image_generator.flow_from_directory(directory=str(data_dir),\n",
    "                                                         batch_size=batch_size,\n",
    "                                                         shuffle=True,\n",
    "                                                         target_size=(img_size, img_size),\n",
    "                                                         classes = list(class_names)\n",
    "                                                        )\n",
    "    if log:\n",
    "        print(\"train_data_gen : \", type(train_data_gen))\n",
    "\n",
    "    return train_data_gen\n",
    "    \n",
    "\n",
    "def show_batch(train_data_gen, class_names=[\"cats\", \"dogs\"]):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    image_data_gen : keras_preprocessing.image.directory_iterator.DirectoryIterator\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    image_data_gen = load_images()\n",
    "    show_batch(image_data_gen)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract batch.\n",
    "    image_batch, label_batch = next(train_data_gen)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5,5, n+1)\n",
    "        plt.imshow(image_batch[n])\n",
    "\n",
    "        # Extract label. \n",
    "        label_index = 0\n",
    "        for index, l in enumerate(label_batch[n]):\n",
    "            if l == 1:\n",
    "                label_index = index\n",
    "                \n",
    "        plt.title(class_names[label_index])\n",
    "        plt.axis('off')\n",
    "        \n",
    "\n",
    "def check_image_data_gen(train_data_gen):\n",
    "    \"\"\"Check data type and shep of train data gen.\"\"\"\n",
    "    image_batch, label_batch = next(train_data_gen)\n",
    "    \n",
    "    print(\"image_batch    : \", type(image_batch), image_batch.shape)\n",
    "    print(\"label_batch    : \", type(label_batch), label_batch.shape)\n",
    "        \n",
    "    print(\"image_batch[0] : \", type(image_batch[0]), image_batch[0].shape)\n",
    "    print(\"label_batch[0] : \", type(label_batch[0]), label_batch[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../images/cats\n",
      "../images/dogs\n",
      "image_count :  2001\n",
      "Found 2001 images belonging to 2 classes.\n",
      "train_data_gen :  <class 'keras_preprocessing.image.directory_iterator.DirectoryIterator'>\n",
      "image_batch    :  <class 'numpy.ndarray'> (32, 224, 224, 3)\n",
      "label_batch    :  <class 'numpy.ndarray'> (32, 2)\n",
      "image_batch[0] :  <class 'numpy.ndarray'> (224, 224, 3)\n",
      "label_batch[0] :  <class 'numpy.ndarray'> (2,)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    CLASS_NAMES=[\"cats\", \"dogs\"]\n",
    "    image_data_gen = load_images_with_image_generator(class_names=CLASS_NAMES)\n",
    "    #show_batch(image_data_gen, class_names=CLASS_NAMES)\n",
    "    check_image_data_gen(image_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
